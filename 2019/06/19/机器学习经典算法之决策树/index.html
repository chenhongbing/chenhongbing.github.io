<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>机器学习经典算法之决策树 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="决策树  决策树构建基本概念 信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数、CART回归树。 决策树构建 ID3—&amp;gt;C4.5—&amp;gt;CART 决策树剪枝 预剪枝、后剪枝 决策树之连续值处理 决策树之缺失值处理   决策树构建 关于决策树构建中信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数的概念可参考链接决策树学习笔记。此外，可结合统计学习方法中的例子进行理解">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习经典算法之决策树">
<meta property="og:url" content="chenhongbing.github.io/2019/06/19/机器学习经典算法之决策树/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="决策树  决策树构建基本概念 信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数、CART回归树。 决策树构建 ID3—&amp;gt;C4.5—&amp;gt;CART 决策树剪枝 预剪枝、后剪枝 决策树之连续值处理 决策树之缺失值处理   决策树构建 关于决策树构建中信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数的概念可参考链接决策树学习笔记。此外，可结合统计学习方法中的例子进行理解">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-07-04T09:23:13.803Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习经典算法之决策树">
<meta name="twitter:description" content="决策树  决策树构建基本概念 信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数、CART回归树。 决策树构建 ID3—&amp;gt;C4.5—&amp;gt;CART 决策树剪枝 预剪枝、后剪枝 决策树之连续值处理 决策树之缺失值处理   决策树构建 关于决策树构建中信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数的概念可参考链接决策树学习笔记。此外，可结合统计学习方法中的例子进行理解">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">孙子兵法</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="chenhongbing.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-机器学习经典算法之决策树" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/19/机器学习经典算法之决策树/" class="article-date">
  <time datetime="2019-06-19T06:04:32.000Z" itemprop="datePublished">2019-06-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习经典算法之决策树
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="决策树"><a class="markdownIt-Anchor" href="#决策树"></a> 决策树</h3>
<ul>
<li>决策树构建基本概念<br>
信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数、CART回归树。</li>
<li>决策树构建<br>
ID3—&gt;C4.5—&gt;CART</li>
<li>决策树剪枝<br>
预剪枝、后剪枝</li>
<li>决策树之连续值处理</li>
<li>决策树之缺失值处理</li>
</ul>
<h3 id="决策树构建"><a class="markdownIt-Anchor" href="#决策树构建"></a> 决策树构建</h3>
<p>关于决策树构建中信息、信息熵、条件熵、信息增益、信息增益率、基尼值、基尼系数的概念可参考链接<a href="https://www.cnblogs.com/houjun/p/8520964.html" target="_blank" rel="noopener">决策树学习笔记</a>。此外，可结合统计学习方法中的例子进行理解。</p>
<ul>
<li>
<p>信息<br>
用概率的负对数来衡量。必然事件，没有信息量；不可能事件，具有无穷大的信息量。</p>
</li>
<li>
<p>信息熵<br>
信息量度量的是一个具体事件发生所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望－－考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。<br>
信息熵可以作为系统复杂程度的度量，如果一个系统越简单，出现情况种类很少(极端情况为1种情况，那么对应概率为1，对应的信息熵为0)，此时信息熵较小；如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。</p>
</li>
<li>
<p>条件熵<br>
在满足某个条件下的熵,比如数据集中有若干变量,计算某个变量的条件熵,为该变量的概率*该变量子集的信息熵。</p>
</li>
<li>
<p>信息增益<br>
特征选择的一个重要指标。信息增益=信息熵-条件熵。</p>
</li>
<li>
<p>信息增益比<br>
信息增益准则对可取值较多的属性有所偏好，为了减少这种偏好可能带来的影响，使用增益率代替信息增益准则选择划分属性。增益率(Gain_ratio(D,a))=信息增益Gain(D,a)/属性固有值(IV(a)).</p>
</li>
<li>
<p>基尼值<br>
基尼值代表了从数据集合D中随机选择两个样本，其类别不一致的概率。基尼值越小，数据集D纯度越高。<br>
当数据集的纯度越高，每次抽到不同类别标记的概率越小。例如，在一个袋子里装100个乒乓球，其中有99个白球，１个黄球，那么当我们随机抽取两个球的时候，很大概率是抽到两个白球。</p>
</li>
<li>
<p>基尼系数<br>
　基尼系数是针对属性定义的，其反映的是，使用属性a进行划分后，所有分支中(使用基尼值度量的)纯度的加权和。<br>
CART分类树根据基尼指数划分属性，选择基尼指数最小的属性作为最优划分属性。</p>
</li>
<li>
<p>特征选择<br>
ID3：信息增益；C4.5：信息增益比；CART分类：基尼系数；CART回归：平方误差最小化.</p>
</li>
<li>
<p>算法思想及改进路线<br>
ID3—&gt;C4.5—&gt;CART<br>
ID3:<br>
(1)如果D中所有实例属于同一类Ck，则T为单节点树，并将类Ck作为该节点的类标记，返回T;<br>
(2)如果特征集A=空集，则T为单节点树，并将D中实例数最大的类Ck作为该节点的类标记，返回T;<br>
(3)否则，求出数据集D中各特征A的信息增益，选择信息增益最大的特征Ag;<br>
(4)如果Ag的信息增益小于阀值e，则停止运算，将D中实例数最大的类Ck作为该节点的类标记;<br>
(5)否则，对Ag的每一个可能值ai，依Ag=ai将D分割为若干非空子集Di，如果子集Di中所有样本均属于同一类，则将该节点置为叶子节点；否则继续(6).<br>
(6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用步(1)~步(5)，得到子树Ti.</p>
<p>C4.5:<br>
(1)如果D中所有实例属于同一类Ck，则T为单节点树，并将类Ck作为该节点的类标记，返回T;<br>
(2)如果特征集A=空集，则T为单节点树，并将D中实例数最大的类Ck作为该节点的类标记，返回T;<br>
(3)否则，求出数据集D中各特征A的信息增益比，选择信息增益比最大的特征Ag;<br>
(4)如果Ag的信息增益比小于阀值e，则停止运算，将D中实例数最多的类Ck作为该节点的类标记;<br>
(5)否则，对Ag的每一个可能值ai，依Ag=ai将D分割为若干非空子集Di，如果子集Di中所有样本均属于同一类，则将该节点置为叶子节点；否则继续(6)<br>
(6)对结点i，以Di为训练集，以A-{Ai}为特征集，递归调用步(1)~步(5)，得到子树Ti.</p>
<p>CART分类树:<br>
(1)对于训练数据集D，计算数据集中每一个特征(A、B、C…)的基尼指数。以特征A为例，A有属性a、b、c；根据属性a，将数据集D分割成D1(是)和D2(否)两部分；然后根据基尼指数定义公式计算A=a时的基尼指数。<br>
(2)遍历计算所有可能的特征(A、B、C…)及每个特征对应的属性的基尼指数。选择基尼指数最小的特征和属性作为最优切分点。根据最优特征与最优切分点，将现结点分割成两个子结点，将训练数据集依特征分配到两个子结点中取。<br>
(3)对两个子结点递归调用(1)、(2),(其中特征包含上述已经计算过基尼指数的特征，需去除已经计算过的属性)，直至满足停止条件(结点中样本个数小于预定阀值、样本集的基尼指数小于预定阀值(样本基本属于同一类)、没有更多特征)。<br>
(4)生成CART分类决策树。</p>
</li>
<li>
<p>CART回归树<br>
CART回归树为二叉树，内部结点特征的取值为&quot;是&quot;和&quot;否&quot;，左分支是取值为&quot;是&quot;的分支，右分支是取值为否的分支。决策树递归地二分每个属性的特征。决策树的构建基于平方误差最小化准则。生成步骤如下:<br>
(1)选择最优切分变量j与切分点s，求解；根据训练数据取值范围，枚举训练数据可能的数据切分点。对各切分点，求出相应的R1、R2，C1、C2；最后通过均方误差求得m(s)。<br>
(2)用选定的(j,s)对划分区域并决定相应的输出值；比较上述求得的m(s)大小结果，取最小值，根据划分区间R1、R2求得C1、C2，然后构建回归树T1(x)。<br>
(3)继续对两个子区域调用步骤1，直至满足停止条件。<br>
(4)将输入空间划分为M个区域R1,R2,…,Rm生成决策树。<br>
　树的根节点分成2支后，再分别在这2支上做分支，以此递推，最终生成一颗完整的决策树；后续再剪枝。原理与实例参考链接，<a href="https://blog.csdn.net/aaa_aaa1sdf/article/details/81588382" target="_blank" rel="noopener">CART回归树原理及示例</a>。</p>
</li>
<li>
<p>GBDT<br>
　 GBDT思想，获得一颗二叉树后，利用残差，再在完整的数据集上生成一颗二叉树，不断迭代，最终将多颗二叉树累加组合成一个最终的函数。GBDT的生成步骤如下:<br>
(1)借用CART回归树的方法生成一颗二叉树<br>
(2)用原始数据与二叉树做差，获得残差<br>
(3)对该残差同CART的方法生成子树，然后相加这两颗树，得到相应的函数<br>
(4)求原始数据与生成树的平方损失误差，如果达到结束条件，则该树为最后生成的树，否则迭代(2)、(3)、(4)步骤</p>
</li>
</ul>
<h3 id="决策树剪枝"><a class="markdownIt-Anchor" href="#决策树剪枝"></a> 决策树剪枝</h3>
<p>决策树算法为了尽可能正确的分类训练样本，不停对结点进行划分，最后会形成这棵树的分支过多，导致过拟合现象。为了避免这种现象的发生，会对树进行剪枝操作。详细内容可参考链接，<a href="https://blog.csdn.net/u012328159/article/details/79285214" target="_blank" rel="noopener">决策树剪枝</a>。决策树的剪枝通常有两种方法，预剪枝和后剪枝。</p>
<ul>
<li>
<p>预剪枝<br>
　预剪枝，在构造决策树的过程中，先对每个结点在划分前进行估计，如果当前结点的划分不能带来决策树模型泛化性能的提升，则不对当前结点进行划分并且将当前结点标记为叶节点。此时可能存在不同类别的样本同时存于结点中，按照多数投票的原则判断该节点所属类别。预剪枝对于何时停止决策树的生长有以下几种方法。<br>
(1)当树到达一定深度的时候，停止树的生长。<br>
(2)当到达当前结点的样本数量小于某个阀值的时候，停止树的生长。<br>
(3)计算每次分裂对测试集的准确度提升，当小于某个阀值的时候，不再继续扩展。<br>
　不足之处：预剪枝使得决策树的很多分支都没有展开，降低了过拟合的风险，显著减少了决策树的训练时间开销和测试时间开销；但是，预剪枝虽然当前不能提升泛化性能，但是基于该划分的后续划分却有可能导致性能的提升，因此预剪枝会加大欠拟合的风险。</p>
</li>
<li>
<p>后剪枝<br>
　后剪枝，先构造完整颗决策树，然后自底向上的对非叶节点进行考察，若该结点对应的子树换成叶节点能够带来泛化性能的提升，则把该子树替换为叶结点。<br>
(1)对于未剪枝的决策树中的每一个内部节点按照自上而下、自左向右的顺序进行编号<br>
(2)从最大编号至最小编号方向进行评估，如果将该编号的子树替换为叶节点(子树中最多的类)，在验证集上会带来泛化能力的提升，则替换子树为叶节点<br>
(3)按照(2)步骤遍历所有的节点<br>
　不足之处:后剪枝决策树相比于预剪枝决策树保留了更多的分支，欠拟合风险小，泛化性能优于预剪枝决策树。其训练时间比未剪枝决策树和预剪枝决策树要大很多。</p>
</li>
</ul>
<h3 id="连续值处理"><a class="markdownIt-Anchor" href="#连续值处理"></a> 连续值处理</h3>
<p>机器学习任务中会遇到连续属性，关于连续数据处理参考链接，<a href="https://blog.csdn.net/u012328159/article/details/79396893" target="_blank" rel="noopener">连续值处理</a>。<br>
(1)给定训练集D和连续属性a。将属性a在D上出现的n个不同取值，按照从小到大的顺序进行排序。<br>
(2)考察n-1个元素的候选划分点集合，划分点的取值为(ai+ai-1)/2。<br>
(3)计算连续属性在每个划分点处的信息增益，选取划分点处信息增益最大的作为与其它属性进行比较。<br>
(4)将(3)中最大的信息增益作为划分点，并重复(1)-(3)步骤，直至满足停止划分条件。<br>
划分停止条件同决策树停止条件。</p>
<h3 id="缺失值处理"><a class="markdownIt-Anchor" href="#缺失值处理"></a> 缺失值处理</h3>
<p>现实生活中的数据集样本，通常在某些属性上是缺失的。如果属性值缺失的样本数量比较少，可以直接简单粗暴的把不完备的样本删除掉；但是，如果有大量的样本都有属性值的缺失，假如删除了大量的样本，会让生成的机器学习模型损失大量有用信息，训练出来的模型性能会受到影响。关于缺失数据处理，参考链接，<a href="https://blog.csdn.net/u012328159/article/details/79413610" target="_blank" rel="noopener">缺失数据处理</a></p>
<p>(1)计算信息增益时，面对属性值缺失的样本如何处理?<br>
　如果属性值缺失的样本数量比较少，直接简单粗暴的把不完备的样本删除掉；但是如果有大量样本都有属性值的缺失，此时，如果删除，会导致机器学习模型损失大量有用的信息，训练出来的模型性能会受到影响。下面介绍大量样本都有属性(a、b、c…)缺失的情况，计算方法。<br>
a.对于属性a，取出样本集D中属性没有缺失的样本子集D1<br>
b.计算属性a样本子集D1的信息增益Ent<br>
c.计算属性a中没有缺失值的样本子集数目D1占总样本集D中数目的比例p，最终属性a的信息增益为p*Ent<br>
d.对样本集D中的每一属性(b、c…)，按照a、b、c的步骤计算其信息增益<br>
e.比较所有属性的信息增益值，确定信息增益值最大的为划分属性点</p>
<p>(2)如何将缺失属性的样本的结点，划分到子结点中去？<br>
缺失属性样本会被同时划分进所有分支里去，在每个分支里权值的大小有所不同。权值大小为未缺失样本划进该分支的样本数量除以未缺失总样本数量。</p>
<p>(3)后续决策树的递归是如何进行的？<br>
　其计算步骤同(1)，不过对于概率的计算，注意缺失样本的权值，以缺失样本的权重进行计算。需注意的一点是对于缺失样本的权重，往下传递，该层缺失样本的权值*上层缺失样本的权值。</p>
<p>(4)对于测试集中的样本含有缺失值该怎么处理？<br>
a.如果有专门处理缺失值的分支，就走这个分支<br>
b.同时探查所有的分支，并组合他们的结果来得到类别对应的概率<br>
　参考链接，<a href="https://blog.csdn.net/leaf_zizi/article/details/83503167" target="_blank" rel="noopener">决策树测试集样本缺失处理</a>。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="chenhongbing.github.io/2019/06/19/机器学习经典算法之决策树/" data-id="cjy1hw4is002w3qjhpvua1j93" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/27/机器学习经典算法之逻辑斯蒂回归/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习经典算法之逻辑斯蒂回归
        
      </div>
    </a>
  
  
    <a href="/2019/06/15/深度学习轻量级模型之MobileNet-v2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">MobileNet v2</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/FlyAI/">FlyAI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo-环境搭建/">Hexo 环境搭建</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hello-world/">hello-world</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习面试总结/">深度学习面试总结</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/目标分类/">目标分类</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/目标检测/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/轻量级模型/">轻量级模型</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learing/">deep learing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node-js/">node.js</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/one-stage/">one-stage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/two-stage/">two-stage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图像分类/">图像分类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/目标检测/">目标检测</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/deep-learing/" style="font-size: 10px;">deep learing</a> <a href="/tags/node-js/" style="font-size: 12.5px;">node.js</a> <a href="/tags/one-stage/" style="font-size: 15px;">one-stage</a> <a href="/tags/two-stage/" style="font-size: 17.5px;">two-stage</a> <a href="/tags/图像分类/" style="font-size: 20px;">图像分类</a> <a href="/tags/目标检测/" style="font-size: 12.5px;">目标检测</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/11/机器学习经典算法之线性回归/">机器学习经典算法之线性回归</a>
          </li>
        
          <li>
            <a href="/2019/07/09/深度学习面试之BP/">深度学习面试之BP</a>
          </li>
        
          <li>
            <a href="/2019/07/05/机器学习经典算法之偏差方差/">机器学习经典算法之偏差方差</a>
          </li>
        
          <li>
            <a href="/2019/06/29/深度学习面试之Python/">深度学习面试之Python</a>
          </li>
        
          <li>
            <a href="/2019/06/27/机器学习经典算法之提升方法/">机器学习经典算法之提升方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Hexo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8249878f4836242732b5440d03f198f6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </div>
</body>
</html>
